{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=_loss, optimizer=_optimizer, metrics=[custom_loss_wrapper_2(model.input)])\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "_activation = Activation('softmax')\n",
    "_optimizer = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "def custom_loss_wrapper_2(inputs):\n",
    "    print(\"inputs {}\".format(inputs.shape))\n",
    "    # source: https://stackoverflow.com/questions/55445712/custom-loss-function-in-keras-based-on-the-input-data\n",
    "    # 2nd source: http://stackoverflow.com/questions.55597335/how-to-use-tf-gather-in-batch\n",
    "    def reindex(tensor_tuple):\n",
    "        # unpack tensor tuple\n",
    "        y_true = tensor_tuple[0]\n",
    "        y_pred = tensor_tuple[1]\n",
    "        t_inputs = K.cast(tensor_tuple[2], dtype='int64')\n",
    "        t_max_indices = K.tf.where(K.tf.equal(t_inputs, K.max(t_inputs)))\n",
    "\n",
    "        # gather the values from y_true and y_pred\n",
    "        print(\"y_true {}\".format(y_true.shape))\n",
    "        print(\"y_pred {}\".format(y_pred.shape))\n",
    "        y_true_gathered = K.gather(y_true, t_max_indices)\n",
    "        y_pred_gathered = K.gather(y_pred, t_max_indices)\n",
    "\n",
    "        print(K.mean(K.square(y_true_gathered - y_pred_gathered)))\n",
    "\n",
    "        return K.mean(K.square(y_true_gathered - y_pred_gathered))\n",
    "\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        print(\"y_true2 {}\".format(y_true.shape))\n",
    "        print(\"y_pred2 {}\".format(y_pred.shape))\n",
    "\n",
    "        # Step 1: \"tensorize\" the previous list\n",
    "        t_inputs = K.variable(inputs)\n",
    "\n",
    "        # Step 2: Stack tensors\n",
    "        tensor_tuple = K.stack([y_true, y_pred, t_inputs], axis=1)\n",
    "\n",
    "        vals = K.map_fn(reindex, tensor_tuple, dtype='float32')\n",
    "        print('vals: {}'.format(vals.shape))\n",
    "        print('kvals: {}'.format(K.mean(vals).shape))\n",
    "        return K.mean(vals, keepdims=True)\n",
    "\n",
    "    return custom_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning.. Using pgf backend, no GUI available. use plt.savefig() for inpection\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Convolutional Long-Short Term Model.\n",
    "\"\"\"\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from sclouds.helpers import get_lon_array, get_lat_array, path_convlstm_results\n",
    "from sclouds.ml.ConvLSTM.utils import r2_keras\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "#my_callbacks = [\n",
    "    #tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "#]\n",
    "#model.fit(dataset, epochs=10, callbacks=my_callbacks)\n",
    "\n",
    "class ConvLSTM:\n",
    "    \"\"\" A convoliutional lstm neural network.\n",
    "\n",
    "    What about :\n",
    "        recurrent_activation='hard_sigmoid'\n",
    "        activation='tanh'\n",
    "\n",
    "    Notes\n",
    "    ----------------------------------------------------------------------------\n",
    "    filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
    "    dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "    use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "    unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None,\n",
    "    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "    recurrent_constraint=None, bias_constraint=None, return_sequences=False,\n",
    "    go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0\n",
    "\n",
    "    (x=x, y=y, batch_size=None, epochs=1, verbose=1, callbacks=None,\n",
    "    validation_split=0.2, validation_data=None, shuffle=True, class_weight=None,\n",
    "    sample_weight=None, initial_epoch=0, steps_per_epoch=None,\n",
    "    validation_steps=None, validation_batch_size=None, validation_freq=1,\n",
    "    max_queue_size=10,\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    DATA_FORMAT        = 'channels_last'\n",
    "    PADDING            = 'same'\n",
    "    RETURN_SEQUENCE    = True\n",
    "    NUM_INPUT_VARS     = 4\n",
    "    OUTPUT_KERNEL_SIZE = 1\n",
    "    OUTPUT_FILTER      = 1\n",
    "    KERNAL_INIT        = 'lecun_uniform'\n",
    "\n",
    "    n_lat   = 81\n",
    "    n_lon   = 161\n",
    "    WORKERS = 16 # identical to the number of cores requested in\n",
    "\n",
    "    USE_MULTIPROCESSING = True\n",
    "    #early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    #CALLBACKS = [early_stopping_monitor, TensorBoard(log_dir='./logs')]\n",
    "\n",
    "    def __init__(self, X_train, y_train, filters, kernels, seq_length = 24,\n",
    "                 epochs=40, batch_size = 20, validation_split=0.1, name = None, result_path = None):\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernels = kernels\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.validation_split = validation_split\n",
    "        print('Starts to build model ...')\n",
    "        self.model = self.build_model(filters, kernels, seq_length)\n",
    "        print('Statrs compilation of model ...')\n",
    "        self.name = name\n",
    "        self.result_path = '/home/hanna/'\n",
    "        self.model.compile(optimizer=keras.optimizers.Adam(\n",
    "                            learning_rate=0.001,\n",
    "                            beta_1=0.9,\n",
    "                            beta_2=0.999,\n",
    "                            epsilon=1e-07,\n",
    "                            amsgrad=False,\n",
    "                            name=\"Adam\",),\n",
    "                            loss='mean_squared_error',\n",
    "                            metrics=['mean_squared_error', r2_keras])\n",
    "        print('starts training')\n",
    "        self.history = self.model.fit(X_train, y_train, batch_size=batch_size,\n",
    "                                     epochs=epochs, verbose=1,\n",
    "                                     #callbacks=self.CALLBACKS,\n",
    "                                     #validation_split=self.validation_split,\n",
    "                                     #validation_data=None,\n",
    "                                     shuffle=False,\n",
    "                                     #class_weight=None,\n",
    "                                     #sample_weight=None, initial_epoch=0,\n",
    "                                     #steps_per_epoch=100,\n",
    "                                     #validation_steps=None,\n",
    "                                     #validation_freq=1, max_queue_size=10,\n",
    "                                     workers=self.WORKERS,\n",
    "                                     use_multiprocessing= self.USE_MULTIPROCESSING)\n",
    "        self.store_history()\n",
    "        self.store_summary()\n",
    "        print('finished model -- ')\n",
    "\n",
    "\n",
    "    def build_model(self, filters, kernels, seq_length = 24):\n",
    "        \"\"\"\" Building a ConvLSTM model for predicting cloud cover.\n",
    "        All filters are squared. Adding the architecture.\n",
    "\n",
    "        Parameteres\n",
    "        ------------------------\n",
    "        filters : array like\n",
    "            use length of this to infer the depth of the network.\n",
    "\n",
    "        Returns\n",
    "        ------------------------\n",
    "        model : tensorflow.keras.Sequential\n",
    "            Builded model\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        model =  keras.Sequential()\n",
    "\n",
    "        model.add( keras.layers.Input(batch_input_shape=(self.batch_size, seq_length, self.n_lat, self.n_lon,\n",
    "                                self.NUM_INPUT_VARS), name='input'))        #batch_size = self.batch_size)\n",
    "\n",
    "        # Adding the first layer\n",
    "        model.add(keras.layers.ConvLSTM2D(filters = filters[0],\n",
    "                           kernel_size = (kernels[0], kernels[0]), #, self.NUM_INPUT_VARS\n",
    "                           input_shape = (seq_length,\n",
    "                                            self.n_lat, self.n_lon, self.NUM_INPUT_VARS),\n",
    "                           kernel_initializer=self.KERNAL_INIT,\n",
    "                           padding = self.PADDING,\n",
    "                           return_sequences=self.RETURN_SEQUENCE,\n",
    "                           data_format=self.DATA_FORMAT,\n",
    "                           batch_size = self.batch_size))\n",
    "\n",
    "        prev_filter = filters[0]\n",
    "        if len(filters) > 1 and len(kernels) > 1:\n",
    "            print('Detected more than one layer ... ')\n",
    "            for i, tuple in enumerate(zip(filters[1:], kernels[1:])):\n",
    "                filter, kernal = tuple\n",
    "                # Begin with 3D convolutional LSTM layer\n",
    "                model.add(keras.layers.ConvLSTM2D(filters=filter,\n",
    "                                                kernel_size=(kernal, kernal), # prev_filter\n",
    "                                                input_shape = (seq_length, self.n_lat,\n",
    "                                                                self.n_lon, prev_filter),\n",
    "                                                kernel_initializer=self.KERNAL_INIT,\n",
    "                                                padding = self.PADDING,\n",
    "                                                return_sequences=self.RETURN_SEQUENCE,\n",
    "                                                data_format=self.DATA_FORMAT,\n",
    "                                                batch_size = self.batch_size))\n",
    "                prev_filter = filter\n",
    "        # Adding the last layer\n",
    "        model.add(keras.layers.ConvLSTM2D(filters=self.OUTPUT_FILTER,\n",
    "                                        kernel_size=(self.OUTPUT_KERNEL_SIZE, self.OUTPUT_KERNEL_SIZE), #prev_filter\n",
    "                                        input_shape = (seq_length, self.n_lat,\n",
    "                                                        self.n_lon, prev_filter),\n",
    "                                        kernel_initializer=self.KERNAL_INIT,\n",
    "                                        padding = self.PADDING,\n",
    "                                        return_sequences=self.RETURN_SEQUENCE,\n",
    "                                        data_format=self.DATA_FORMAT,\n",
    "                                        batch_size = self.batch_size))\n",
    "\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def compile(self, lmd=0.001):\n",
    "        \"\"\" Compile model.\n",
    "\n",
    "        Parameters\n",
    "        -------------\n",
    "        model : tensorflow.keras.Sequential\n",
    "            Build model.\n",
    "\n",
    "        Returnes\n",
    "        -------------\n",
    "        model : tensorflow.keras.Sequential\n",
    "            Compiled model.\n",
    "        \"\"\"\n",
    "        _loss = custom_loss_wrapper_2(self.model.inputs)\n",
    "        self.model.compile(optimizer=keras.optimizers.Adam(\n",
    "                            learning_rate=lmd,\n",
    "                            beta_1=0.9,\n",
    "                            beta_2=0.999,\n",
    "                            epsilon=1e-07,\n",
    "                            amsgrad=False,\n",
    "                            name=\"Adam\",),\n",
    "                            loss= _loss,\n",
    "                            metrics= [_loss])\n",
    "        return self.model\n",
    "\n",
    "\n",
    "\n",
    "    def store_history(self):\n",
    "        \"\"\" Fit builded model.\n",
    "        Parameters\n",
    "        -------------\n",
    "        model : tensorflow.keras.Sequential\n",
    "            Builded model\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        history = self.history\n",
    "\n",
    "        # convert the history.history dict to a pandas DataFrame:\n",
    "        hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "        # save to json:\n",
    "        hist_json_file = os.path.join(self.result_path, 'history.json')\n",
    "        with open(hist_json_file, mode='w') as f:\n",
    "            hist_df.to_json(f)\n",
    "\n",
    "        # or save to csv:\n",
    "        hist_csv_file = os.path.join(self.result_path, 'history.csv')\n",
    "        with open(hist_csv_file, mode='w') as f:\n",
    "            hist_df.to_csv(f)\n",
    "\n",
    "        return\n",
    "\n",
    "    def store_summary(self):\n",
    "        \"\"\" Store summary of tranings process.\n",
    "        \"\"\"\n",
    "        ORIG_OUTPUT = sys.stdout\n",
    "        with open(os.path.join(self.result_path, \"summary_{}.txt\".format(self.name)), \"w\") as text_file:\n",
    "            sys.stdout = text_file\n",
    "            self.model.summary()\n",
    "        sys.stdout = ORIG_OUTPUT\n",
    "        self.model.save(os.path.join(self.result_path,'{}.h5'.format(self.name)))  # creates a HDF5 file 'my_model.h5'\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts to build model ...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d (ConvLSTM2D)    (2, 24, 81, 161, 8)       3488      \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_1 (ConvLSTM2D)  (2, 24, 81, 161, 1)       40        \n",
      "=================================================================\n",
      "Total params: 3,528\n",
      "Trainable params: 3,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Statrs compilation of model ...\n",
      "starts training\n",
      "Train on 4 samples\n",
      "Epoch 1/40\n",
      "4/4 [==============================] - ETA: 7s - loss: 0.7235 - mean_squared_error: 0.7235 - r2_keras: -4528962797568.00 - 11s 3s/sample - loss: 0.7002 - mean_squared_error: 0.7002 - r2_keras: -4383228035072.0000\n",
      "Epoch 2/40\n",
      "4/4 [==============================] - ETA: 2s - loss: 0.6352 - mean_squared_error: 0.6352 - r2_keras: -3976012496896.00 - 6s 2s/sample - loss: 0.6174 - mean_squared_error: 0.6174 - r2_keras: -3864557780992.0000\n",
      "Epoch 3/40\n",
      "4/4 [==============================] - ETA: 4s - loss: 0.5702 - mean_squared_error: 0.5702 - r2_keras: -3569358209024.00 - 8s 2s/sample - loss: 0.5583 - mean_squared_error: 0.5583 - r2_keras: -3494515834880.0000\n",
      "Epoch 4/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.5266 - mean_squared_error: 0.5266 - r2_keras: -3296579289088.00 - 7s 2s/sample - loss: 0.5183 - mean_squared_error: 0.5183 - r2_keras: -3244692078592.0000\n",
      "Epoch 5/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.4957 - mean_squared_error: 0.4957 - r2_keras: -3102650925056.00 - 6s 2s/sample - loss: 0.4892 - mean_squared_error: 0.4892 - r2_keras: -3062403694592.0000\n",
      "Epoch 6/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.4711 - mean_squared_error: 0.4711 - r2_keras: -2948761124864.00 - 8s 2s/sample - loss: 0.4656 - mean_squared_error: 0.4656 - r2_keras: -2914810593280.0000\n",
      "Epoch 7/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.4501 - mean_squared_error: 0.4501 - r2_keras: -2817354891264.00 - 7s 2s/sample - loss: 0.4453 - mean_squared_error: 0.4453 - r2_keras: -2787443212288.0000\n",
      "Epoch 8/40\n",
      "4/4 [==============================] - ETA: 4s - loss: 0.4315 - mean_squared_error: 0.4315 - r2_keras: -2700948275200.00 - 6s 2s/sample - loss: 0.4272 - mean_squared_error: 0.4272 - r2_keras: -2674101583872.0000\n",
      "Epoch 9/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.4148 - mean_squared_error: 0.4148 - r2_keras: -2596219912192.00 - 8s 2s/sample - loss: 0.4109 - mean_squared_error: 0.4109 - r2_keras: -2571956912128.0000\n",
      "Epoch 10/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.3996 - mean_squared_error: 0.3996 - r2_keras: -2501536907264.00 - 7s 2s/sample - loss: 0.3961 - mean_squared_error: 0.3961 - r2_keras: -2479573434368.0000\n",
      "Epoch 11/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.3859 - mean_squared_error: 0.3859 - r2_keras: -2415793012736.00 - 8s 2s/sample - loss: 0.3827 - mean_squared_error: 0.3827 - r2_keras: -2395881078784.0000\n",
      "Epoch 12/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.3735 - mean_squared_error: 0.3735 - r2_keras: -2338030616576.00 - 8s 2s/sample - loss: 0.3706 - mean_squared_error: 0.3706 - r2_keras: -2319943729152.0000\n",
      "Epoch 13/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.3622 - mean_squared_error: 0.3622 - r2_keras: -2267342438400.00 - 8s 2s/sample - loss: 0.3596 - mean_squared_error: 0.3596 - r2_keras: -2250860134400.0000\n",
      "Epoch 14/40\n",
      "4/4 [==============================] - ETA: 4s - loss: 0.3519 - mean_squared_error: 0.3519 - r2_keras: -2202848985088.00 - 8s 2s/sample - loss: 0.3495 - mean_squared_error: 0.3495 - r2_keras: -2187792220160.0000\n",
      "Epoch 15/40\n",
      "4/4 [==============================] - ETA: 4s - loss: 0.3425 - mean_squared_error: 0.3425 - r2_keras: -2143915606016.00 - 7s 2s/sample - loss: 0.3403 - mean_squared_error: 0.3403 - r2_keras: -2130046091264.0000\n",
      "Epoch 16/40\n",
      "4/4 [==============================] - ETA: 4s - loss: 0.3339 - mean_squared_error: 0.3339 - r2_keras: -2090147512320.00 - 8s 2s/sample - loss: 0.3319 - mean_squared_error: 0.3319 - r2_keras: -2077590552576.0000\n",
      "Epoch 17/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.3260 - mean_squared_error: 0.3260 - r2_keras: -2040603213824.00 - 8s 2s/sample - loss: 0.3241 - mean_squared_error: 0.3241 - r2_keras: -2028684705792.0000\n",
      "Epoch 18/40\n",
      "4/4 [==============================] - ETA: 2s - loss: 0.3185 - mean_squared_error: 0.3185 - r2_keras: -1993441017856.00 - 6s 1s/sample - loss: 0.3166 - mean_squared_error: 0.3166 - r2_keras: -1981998432256.0000\n",
      "Epoch 19/40\n",
      "4/4 [==============================] - ETA: 4s - loss: 0.3112 - mean_squared_error: 0.3112 - r2_keras: -1948048687104.00 - 7s 2s/sample - loss: 0.3094 - mean_squared_error: 0.3094 - r2_keras: -1936952131584.0000\n",
      "Epoch 20/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.3042 - mean_squared_error: 0.3042 - r2_keras: -1903917137920.00 - 6s 2s/sample - loss: 0.3024 - mean_squared_error: 0.3024 - r2_keras: -1893052186624.0000\n",
      "Epoch 21/40\n",
      "4/4 [==============================] - ETA: 2s - loss: 0.2972 - mean_squared_error: 0.2972 - r2_keras: -1860609638400.00 - 5s 1s/sample - loss: 0.2955 - mean_squared_error: 0.2955 - r2_keras: -1849878642688.0000\n",
      "Epoch 22/40\n",
      "4/4 [==============================] - ETA: 2s - loss: 0.2904 - mean_squared_error: 0.2904 - r2_keras: -1817772425216.00 - 7s 2s/sample - loss: 0.2887 - mean_squared_error: 0.2887 - r2_keras: -1807123218432.0000\n",
      "Epoch 23/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.2836 - mean_squared_error: 0.2836 - r2_keras: -1775541288960.00 - 7s 2s/sample - loss: 0.2824 - mean_squared_error: 0.2824 - r2_keras: -1767702003712.0000\n",
      "Epoch 24/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.2787 - mean_squared_error: 0.2787 - r2_keras: -1744757325824.00 - 8s 2s/sample - loss: 0.2775 - mean_squared_error: 0.2775 - r2_keras: -1737211772928.0000\n",
      "Epoch 25/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.2739 - mean_squared_error: 0.2739 - r2_keras: -1714760318976.00 - 8s 2s/sample - loss: 0.2728 - mean_squared_error: 0.2728 - r2_keras: -1707337580544.0000\n",
      "Epoch 26/40\n",
      "4/4 [==============================] - ETA: 2s - loss: 0.2692 - mean_squared_error: 0.2692 - r2_keras: -1685049966592.00 - 6s 1s/sample - loss: 0.2680 - mean_squared_error: 0.2680 - r2_keras: -1677576372224.0000\n",
      "Epoch 27/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.2644 - mean_squared_error: 0.2644 - r2_keras: -1655021895680.00 - 8s 2s/sample - loss: 0.2632 - mean_squared_error: 0.2632 - r2_keras: -1647403859968.0000\n",
      "Epoch 28/40\n",
      "4/4 [==============================] - ETA: 2s - loss: 0.2595 - mean_squared_error: 0.2595 - r2_keras: -1624357208064.00 - 6s 2s/sample - loss: 0.2582 - mean_squared_error: 0.2582 - r2_keras: -1616539811840.0000\n",
      "Epoch 29/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.2545 - mean_squared_error: 0.2545 - r2_keras: -1593019072512.00 - 7s 2s/sample - loss: 0.2532 - mean_squared_error: 0.2532 - r2_keras: -1585100619776.0000\n",
      "Epoch 30/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.2494 - mean_squared_error: 0.2494 - r2_keras: -1561217204224.00 - 8s 2s/sample - loss: 0.2481 - mean_squared_error: 0.2481 - r2_keras: -1553088905216.0000\n",
      "Epoch 31/40\n",
      "4/4 [==============================] - ETA: 2s - loss: 0.2441 - mean_squared_error: 0.2441 - r2_keras: -1528272519168.00 - 6s 2s/sample - loss: 0.2428 - mean_squared_error: 0.2428 - r2_keras: -1519692808192.0000\n",
      "Epoch 32/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.2386 - mean_squared_error: 0.2386 - r2_keras: -1493417984000.00 - 9s 2s/sample - loss: 0.2371 - mean_squared_error: 0.2371 - r2_keras: -1484269944832.0000\n",
      "Epoch 33/40\n",
      "4/4 [==============================] - ETA: 4s - loss: 0.2326 - mean_squared_error: 0.2326 - r2_keras: -1456157622272.00 - 8s 2s/sample - loss: 0.2311 - mean_squared_error: 0.2311 - r2_keras: -1446310707200.0000\n",
      "Epoch 34/40\n",
      "4/4 [==============================] - ETA: 4s - loss: 0.2262 - mean_squared_error: 0.2262 - r2_keras: -1415964655616.00 - 7s 2s/sample - loss: 0.2245 - mean_squared_error: 0.2245 - r2_keras: -1405277700096.0000\n",
      "Epoch 35/40\n",
      "4/4 [==============================] - ETA: 2s - loss: 0.2192 - mean_squared_error: 0.2192 - r2_keras: -1372273377280.00 - 7s 2s/sample - loss: 0.2174 - mean_squared_error: 0.2174 - r2_keras: -1360632872960.0000\n",
      "Epoch 36/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.2117 - mean_squared_error: 0.2117 - r2_keras: -1324961759232.00 - 7s 2s/sample - loss: 0.2097 - mean_squared_error: 0.2097 - r2_keras: -1312405585920.0000\n",
      "Epoch 37/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.2035 - mean_squared_error: 0.2035 - r2_keras: -1273657032704.00 - 6s 2s/sample - loss: 0.2013 - mean_squared_error: 0.2013 - r2_keras: -1260027117568.0000\n",
      "Epoch 38/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.1946 - mean_squared_error: 0.1946 - r2_keras: -1218151186432.00 - 7s 2s/sample - loss: 0.1923 - mean_squared_error: 0.1923 - r2_keras: -1203597869056.0000\n",
      "Epoch 39/40\n",
      "4/4 [==============================] - ETA: 4s - loss: 0.1852 - mean_squared_error: 0.1852 - r2_keras: -1159229472768.00 - 8s 2s/sample - loss: 0.1828 - mean_squared_error: 0.1828 - r2_keras: -1144076369920.0000\n",
      "Epoch 40/40\n",
      "4/4 [==============================] - ETA: 3s - loss: 0.1755 - mean_squared_error: 0.1755 - r2_keras: -1098397646848.00 - 7s 2s/sample - loss: 0.1730 - mean_squared_error: 0.1730 - r2_keras: -1083174682624.0000\n",
      "finished model -- \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "num_vars = 4\n",
    "# (seq_length, self.n_lat, self.n_lon, self.NUM_INPUT_VARS),\n",
    "seq_length = 24\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 2 #20\n",
    "dummy_num_samples = 50\n",
    "X_train = tf.ones((batch_size*2, seq_length, 81, 161, num_vars))\n",
    "y_train = tf.ones((batch_size*2, seq_length, 81, 161))\n",
    "\n",
    "# antall filrer i hver lag.\n",
    "filters = [8] #256, 128,\n",
    "# size of filters used \n",
    "kernels = [3] #, 3, 3\n",
    "#from utils import get_xarray_dataset_for_period, get_data_keras\n",
    "#data = get_xarray_dataset_for_period(start = '2012-01-01', stop = '2012-01-31')\n",
    "#print(data)\n",
    "#X_train, y_train = get_data_keras(data, num_samples = None, seq_length = 24, batch_size = 10,\n",
    "#                data_format='channels_last')\n",
    "\n",
    "model = ConvLSTM(X_train=X_train, y_train=y_train, filters=filters,\n",
    "                 kernels=kernels, seq_length = seq_length,\n",
    "                 epochs=epochs, batch_size = batch_size, validation_split=0.1,\n",
    "                 name = 'test_model', result_path = '/home/hannasv/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "batch_size = 9\n",
    "seq_length = 24\n",
    "num_vars = 4\n",
    "y_pred  = np.ones((batch_size*2, seq_length, 81, 161))\n",
    "y_train = np.ones((batch_size*2, seq_length, 81, 161))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(y_pred - y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts to build model ...\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_6 (ConvLSTM2D)  (3, 24, 81, 161, 8)       3488      \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_7 (ConvLSTM2D)  (3, 24, 81, 161, 1)       40        \n",
      "=================================================================\n",
      "Total params: 3,528\n",
      "Trainable params: 3,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Statrs compilation of model ...\n",
      "starts training\n",
      "Train on 5 samples, validate on 1 samples\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - ETA: 7s - loss: 1.1709 - mean_squared_error: 1.1709 - r2_keras: -10994623447040.000 - 12s 2s/sample - loss: 1.1709 - mean_squared_error: 1.1709 - r2_keras: -10994623447040.0000\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Specified a list with shape [3,81,161,4] from a tensor with shape [2,81,161,4]\n\t [[node sequential_3/conv_lst_m2d_6/TensorArrayUnstack/TensorListFromTensor (defined at <ipython-input-12-d2624e7a0581>:102) ]] [Op:__inference_distributed_function_22498]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-45dc3a422a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                  \u001b[0mkernels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                  name = 'test_model', result_path = '/home/hannasv/')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-d2624e7a0581>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X_train, y_train, filters, kernels, seq_length, epochs, batch_size, validation_split, name, result_path)\u001b[0m\n\u001b[1;32m    100\u001b[0m                                      \u001b[0;31m#validation_freq=1, max_queue_size=10,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                                      \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWORKERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                                      use_multiprocessing= self.USE_MULTIPROCESSING)\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Specified a list with shape [3,81,161,4] from a tensor with shape [2,81,161,4]\n\t [[node sequential_3/conv_lst_m2d_6/TensorArrayUnstack/TensorListFromTensor (defined at <ipython-input-12-d2624e7a0581>:102) ]] [Op:__inference_distributed_function_22498]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "num_vars = 4\n",
    "# (seq_length, self.n_lat, self.n_lon, self.NUM_INPUT_VARS),\n",
    "\n",
    "seq_length = 24\n",
    "epochs = 10\n",
    "batch_size = 3 #20\n",
    "dummy_num_samples = 50\n",
    "X_train = tf.ones((batch_size*2, seq_length, 81, 161, num_vars))\n",
    "y_train = tf.ones((batch_size*2, seq_length, 81, 161))\n",
    "\n",
    "# antall filrer i hver lag.\n",
    "filters = [8] #256, 128,\n",
    "# size of filters used \n",
    "kernels = [3] #, 3, 3\n",
    "#from utils import get_xarray_dataset_for_period, get_data_keras\n",
    "#data = get_xarray_dataset_for_period(start = '2012-01-01', stop = '2012-01-31')\n",
    "#print(data)\n",
    "#X_train, y_train = get_data_keras(data, num_samples = None, seq_length = 24, batch_size = 10,\n",
    "#                data_format='channels_last')\n",
    "\n",
    "model = ConvLSTM(X_train=X_train, y_train=y_train, filters=filters,\n",
    "                 kernels=kernels, seq_length = seq_length,\n",
    "                 epochs=epochs, batch_size = batch_size, validation_split=0.1,\n",
    "                 name = 'test_model', result_path = '/home/hannasv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
