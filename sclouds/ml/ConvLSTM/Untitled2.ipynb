{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convolutional Long-Short Term Model.\n",
    "\"\"\"\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from sclouds.helpers import get_lon_array, get_lat_array, path_convlstm_results\n",
    "from sclouds.ml.ConvLSTM.utils import r2_keras\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "#my_callbacks = [\n",
    "    #tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "    #tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "#]\n",
    "#model.fit(dataset, epochs=10, callbacks=my_callbacks)\n",
    "\n",
    "class ConvLSTM:\n",
    "    \"\"\" A convoliutional lstm neural network.\n",
    "\n",
    "    What about :\n",
    "        recurrent_activation='hard_sigmoid'\n",
    "        activation='tanh'\n",
    "\n",
    "    Notes\n",
    "    ----------------------------------------------------------------------------\n",
    "    filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
    "    dilation_rate=(1, 1), activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "    use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "    unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None,\n",
    "    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "    recurrent_constraint=None, bias_constraint=None, return_sequences=False,\n",
    "    go_backwards=False, stateful=False, dropout=0.0, recurrent_dropout=0.0\n",
    "\n",
    "    (x=x, y=y, batch_size=None, epochs=1, verbose=1, callbacks=None,\n",
    "    validation_split=0.2, validation_data=None, shuffle=True, class_weight=None,\n",
    "    sample_weight=None, initial_epoch=0, steps_per_epoch=None,\n",
    "    validation_steps=None, validation_batch_size=None, validation_freq=1,\n",
    "    max_queue_size=10,\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    DATA_FORMAT        = 'channels_last'\n",
    "    PADDING            = 'same'\n",
    "    RETURN_SEQUENCE    = True\n",
    "    NUM_INPUT_VARS     = 4\n",
    "    OUTPUT_KERNEL_SIZE = 1\n",
    "    OUTPUT_FILTER      = 1\n",
    "    KERNAL_INIT        = 'lecun_uniform'\n",
    "\n",
    "    n_lat   = 81\n",
    "    n_lon   = 161\n",
    "    WORKERS = 16 # identical to the number of cores requested in\n",
    "\n",
    "    USE_MULTIPROCESSING = True\n",
    "    early_stopping_monitor = EarlyStopping(patience=3)\n",
    "    CALLBACKS = [early_stopping_monitor, TensorBoard(log_dir='./logs')]\n",
    "\n",
    "    def __init__(self, X_train, y_train, filters, kernels, seq_length = 24,\n",
    "                 epochs=40, batch_size = 20, validation_split=0.1, name = None, result_path = None):\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernels = kernels\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.validation_split = validation_split\n",
    "        print('Starts to build model ...')\n",
    "        self.model = self.build_model(filters, kernels, seq_length)\n",
    "        print('Statrs compilation of model ...')\n",
    "        self.name = name\n",
    "        \"\"\"\n",
    "        if result_path is not None:\n",
    "            self.result_path = result_path\n",
    "            if not os.path.exists(result_path):\n",
    "                os.makedirs(result_path)\n",
    "            self.result_path = os.path.join(result_path, name)\n",
    "            if not os.path.exists(os.path.join(result_path, name)):\n",
    "                os.makedirs(os.path.join(result_path, name))\n",
    "        else:\n",
    "            self.result_path = '/home/hannasv/results/'\n",
    "        \"\"\"\n",
    "        self.result_path = '/home/hanna/lagrings/results/'\n",
    "        self.model.compile(optimizer=keras.optimizers.Adam(\n",
    "                            learning_rate=0.001,\n",
    "                            beta_1=0.9,\n",
    "                            beta_2=0.999,\n",
    "                            epsilon=1e-07,\n",
    "                            amsgrad=False,\n",
    "                            name=\"Adam\",),\n",
    "                            loss='mean_squared_error',\n",
    "                            metrics=['mean_squared_error', r2_keras])\n",
    "        print('starts training')\n",
    "        self.history = self.model.fit(X_train, y_train, batch_size=batch_size,\n",
    "                                     epochs=epochs, verbose=1,\n",
    "                                     #callbacks=self.CALLBACKS,\n",
    "                                     #validation_split=self.validation_split,\n",
    "                                     #validation_data=None,\n",
    "                                     shuffle=False,\n",
    "                                     #class_weight=None,\n",
    "                                     #sample_weight=None, initial_epoch=0,\n",
    "                                     #steps_per_epoch=100,\n",
    "                                     #validation_steps=None,\n",
    "                                     #validation_freq=1, max_queue_size=10,\n",
    "                                     workers=self.WORKERS,\n",
    "                                     use_multiprocessing= self.USE_MULTIPROCESSING)\n",
    "        self.store_history()\n",
    "        self.store_summary()\n",
    "        print('finished model -- ')\n",
    "\n",
    "\n",
    "    def build_model(self, filters, kernels, seq_length = 24):\n",
    "        \"\"\"\" Building a ConvLSTM model for predicting cloud cover.\n",
    "        All filters are squared. Adding the architecture.\n",
    "\n",
    "        Parameteres\n",
    "        ------------------------\n",
    "        filters : array like\n",
    "            use length of this to infer the depth of the network.\n",
    "\n",
    "        Returns\n",
    "        ------------------------\n",
    "        model : tensorflow.keras.Sequential\n",
    "            Builded model\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        model =  keras.Sequential()\n",
    "\n",
    "        #input  = keras.layers.Input(batch_input_shape=(self.batch_size, seq_length, self.n_lat, self.n_lon,\n",
    "        #                        self.NUM_INPUT_VARS), name='input')#batch_size = self.batch_size)\n",
    "\n",
    "        # Adding the first layer\n",
    "        model.add(keras.layers.ConvLSTM2D(filters = filters[0],\n",
    "                           kernel_size = (kernels[0], kernels[0]), #, self.NUM_INPUT_VARS\n",
    "                           input_shape = (seq_length,\n",
    "                                            self.n_lat, self.n_lon, self.NUM_INPUT_VARS),\n",
    "                           kernel_initializer=self.KERNAL_INIT,\n",
    "                           padding = self.PADDING,\n",
    "                           return_sequences=self.RETURN_SEQUENCE,\n",
    "                           data_format=self.DATA_FORMAT,\n",
    "                           batch_size = self.batch_size))\n",
    "\n",
    "        prev_filter = filters[0]\n",
    "        if len(filters) > 1 and len(kernels) > 1:\n",
    "            print('Detected more than one layer ... ')\n",
    "            for i, tuple in enumerate(zip(filters[1:], kernels[1:])):\n",
    "                filter, kernal = tuple\n",
    "                # Begin with 3D convolutional LSTM layer\n",
    "                model.add(keras.layers.ConvLSTM2D(filters=filter,\n",
    "                                                kernel_size=(kernal, kernal), # prev_filter\n",
    "                                                #input_shape = (seq_length, self.n_lat,\n",
    "                                                #                self.n_lon, prev_filter),\n",
    "                                                kernel_initializer=self.KERNAL_INIT,\n",
    "                                                padding = self.PADDING,\n",
    "                                                return_sequences=self.RETURN_SEQUENCE,\n",
    "                                                data_format=self.DATA_FORMAT,\n",
    "                                                batch_size = self.batch_size))\n",
    "                prev_filter = filter\n",
    "        # Adding the last layer\n",
    "        model.add(keras.layers.ConvLSTM2D(filters=self.OUTPUT_FILTER,\n",
    "                                        kernel_size=(self.OUTPUT_KERNEL_SIZE, self.OUTPUT_KERNEL_SIZE), #prev_filter\n",
    "                                        #input_shape = (seq_length, self.n_lat,\n",
    "                                        #                self.n_lon, prev_filter),\n",
    "                                        kernel_initializer=self.KERNAL_INIT,\n",
    "                                        padding = self.PADDING,\n",
    "                                        return_sequences=self.RETURN_SEQUENCE,\n",
    "                                        data_format=self.DATA_FORMAT,\n",
    "                                        batch_size = self.batch_size))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def compile(self, lmd=0.001):\n",
    "        \"\"\" Compile model.\n",
    "\n",
    "        Parameters\n",
    "        -------------\n",
    "        model : tensorflow.keras.Sequential\n",
    "            Build model.\n",
    "\n",
    "        Returnes\n",
    "        -------------\n",
    "        model : tensorflow.keras.Sequential\n",
    "            Compiled model.\n",
    "        \"\"\"\n",
    "        self.model.compile(optimizer=keras.optimizers.Adam(\n",
    "                            learning_rate=lmd,\n",
    "                            beta_1=0.9,\n",
    "                            beta_2=0.999,\n",
    "                            epsilon=1e-07,\n",
    "                            amsgrad=False,\n",
    "                            name=\"Adam\",),\n",
    "                            loss='mean_squared_error',\n",
    "                            metrics=['mean_squared_error', r2_keras])\n",
    "        return self.model\n",
    "\n",
    "\n",
    "\n",
    "    def store_history(self):\n",
    "        \"\"\" Fit builded model.\n",
    "        Parameters\n",
    "        -------------\n",
    "        model : tensorflow.keras.Sequential\n",
    "            Builded model\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        history = self.history\n",
    "\n",
    "        # convert the history.history dict to a pandas DataFrame:\n",
    "        hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "        # save to json:\n",
    "        hist_json_file = os.path.join(self.result_path, 'history.json')\n",
    "        with open(hist_json_file, mode='w') as f:\n",
    "            hist_df.to_json(f)\n",
    "\n",
    "        # or save to csv:\n",
    "        hist_csv_file = os.path.join(self.result_path, 'history.csv')\n",
    "        with open(hist_csv_file, mode='w') as f:\n",
    "            hist_df.to_csv(f)\n",
    "\n",
    "        return\n",
    "\n",
    "    def store_summary(self):\n",
    "        \"\"\" Store summary of tranings process.\n",
    "        \"\"\"\n",
    "        ORIG_OUTPUT = sys.stdout\n",
    "        with open(os.path.join(self.result_path, \"summary_{}.txt\".format(self.name)), \"w\") as text_file:\n",
    "            sys.stdout = text_file\n",
    "            self.model.summary()\n",
    "        sys.stdout = ORIG_OUTPUT\n",
    "        self.model.save(os.path.join(self.result_path,'{}.h5'.format(self.name)))  # creates a HDF5 file 'my_model.h5'\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 24, 81, 161, 4)\n",
      "(20, 24, 81, 161)\n",
      "Starts to build model ...\n",
      "Statrs compilation of model ...\n",
      "starts training\n",
      "Train on 20 samples\n",
      "Epoch 1/40\n",
      "20/20 [==============================] - 169s 8s/sample\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[20,81,161,32] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node sequential_2/conv_lst_m2d_4/while/body/_1/clip_by_value_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_13933]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ba7d78ef9f7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                  \u001b[0mkernels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                  name = 'test_model', result_path = '/home/hannasv/results/')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f0743d8daf94>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X_train, y_train, filters, kernels, seq_length, epochs, batch_size, validation_split, name, result_path)\u001b[0m\n\u001b[1;32m    112\u001b[0m                                      \u001b[0;31m#validation_freq=1, max_queue_size=10,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                                      \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWORKERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                                      use_multiprocessing= self.USE_MULTIPROCESSING)\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/anaconda3/envs/final/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[20,81,161,32] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node sequential_2/conv_lst_m2d_4/while/body/_1/clip_by_value_2}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_13933]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "num_vars = 4\n",
    "# (seq_length, self.n_lat, self.n_lon, self.NUM_INPUT_VARS),\n",
    "seq_length = 24\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 20\n",
    "X_train = tf.ones((batch_size, seq_length, 81, 161, num_vars))\n",
    "y_train = tf.ones((batch_size, seq_length, 81, 161))\n",
    "\n",
    "# antall filrer i hver lag.\n",
    "filters = [32] #256, 128,\n",
    "# size of filters used \n",
    "kernels = [3] #, 3, 3\n",
    "\n",
    "from utils import get_xarray_dataset_for_period, get_data_keras\n",
    "#data = get_xarray_dataset_for_period(start = '2012-01-01', stop = '2012-01-31')\n",
    "#print(data)\n",
    "#X_train, y_train = get_data_keras(data, num_samples = None, seq_length = 24, batch_size = 10,\n",
    "#                data_format='channels_last')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "model = ConvLSTM(X_train=X_train, y_train=y_train, filters=filters,\n",
    "                 kernels=kernels, seq_length = seq_length,\n",
    "                 epochs=epochs, batch_size = batch_size, validation_split=0.1,\n",
    "                 name = 'test_model', result_path = '/home/hannasv/results/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
