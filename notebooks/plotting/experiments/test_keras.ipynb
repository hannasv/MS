{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization\n",
    "\n",
    "from sclouds.helpers import path_input\n",
    "from sclouds.io.utils import (dataset_to_numpy, dataset_to_numpy_grid, \n",
    "                              dataset_to_numpy_order, get_xarray_dataset_for_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_xarray_dataset_for_period in module sclouds.io.utils:\n",
      "\n",
      "get_xarray_dataset_for_period(start='2012-01-01', stop='2012-01-31')\n",
      "    Reads data from the requested period into a xarray dataset.\n",
      "    \n",
      "    Parameteres\n",
      "    ----------------------\n",
      "    start : str\n",
      "        Start of period. First day included. (default '2012-01-01')\n",
      "    \n",
      "    stop : str\n",
      "        end of period. Last day included. (default '2012-01-31')\n",
      "    \n",
      "    Returns\n",
      "    -----------------------\n",
      "    data : xr.Dataset\n",
      "        Dataset including all variables in the requested period.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(get_xarray_dataset_for_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files =  ['/home/hanna/lagrings/ERA5_monthly/2012_01_q.nc',\n",
    "          '/home/hanna/lagrings/ERA5_monthly/2012_01_r.nc',\n",
    "          '/home/hanna/lagrings/ERA5_monthly/2012_01_t2m.nc',\n",
    "          '/home/hanna/lagrings/ERA5_monthly/2012_01_sp.nc',\n",
    "          '/home/hanna/lagrings/ERA5_monthly/2012_01_tcc.nc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num files 5\n"
     ]
    }
   ],
   "source": [
    "data = get_xarray_dataset_for_period(start = \"2012-01-01\", stop = \"2012-01-31\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset_to_numpy_grid(data, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((744, 81, 161, 4), (744, 81, 161, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#744/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = X.reshape(186, 4, 81, 161, 4)\n",
    "w = y.reshape(186, 4, 81, 161, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters     = [256, 218, 64, 32]\n",
    "kernel_size = [3, 3, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.layers import keras.layers.\n",
    "\n",
    "seq =  Sequential()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAT = (30,50)\n",
    "LON = (-15,25)\n",
    "SPATIAL_RESOLUTION = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, test_split = 0.2):\n",
    "    assert test_split < 1, 'test split is given as a decimal number, choose a number between 0, 1'\n",
    "    indx = int(len(data)*(1-test_split))\n",
    "    # returns train test\n",
    "    return data[:indx], data[indx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalize(data, seq_len = 4): \n",
    "    samples, metvars, lats, lons = data.shape\n",
    "    #.mean(axis = 0).mean(axis=1).mean(axis=1)\n",
    "    \n",
    "    normalized = np.zeros((samples, metvars, lats, lons))\n",
    "    means   = np.zeros(metvars)\n",
    "    storage = np.zeros(metvars)\n",
    "    \n",
    "    for i in range(metvars):\n",
    "        raveled = data[:, i, :, :].reshape(-1)\n",
    "        m = raveled.mean()\n",
    "        s = raveled.std()\n",
    "        normalized[:, i, :, :] =  (data[:, i, :, :] - m)/s\n",
    "        \n",
    "    samples, metvars, lats, lons = normalized.shape\n",
    "    assert seq_len % 4 == 0\n",
    "    \n",
    "    new_samples = int(samples/seq_len)\n",
    "    normalized  = normalized.reshape( (new_samples, seq_len, metvars, lats, lons ) )\n",
    "    \n",
    "    return normalized, means, storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#from keras.layers import keras.layers.\n",
    "\n",
    "seq =  Sequential()\n",
    "\n",
    "# Begin with 2D convolutional LSTM layer\n",
    "seq.add(ConvLSTM2D(filters = 32, kernel_size = (3, 3),\n",
    "                   input_shape = (4, 81, 161, 4),\n",
    "                   padding = 'same', return_sequences=True, data_format='channels_last')) \n",
    "\n",
    "seq.add(keras.layers.ConvLSTM2D(filters=8, kernel_size=(3, 3),\n",
    "                                  #input_shape=(None, 35, 60, 1),\n",
    "                                  padding='same', return_sequences=True,  data_format='channels_last')) \n",
    "\n",
    "# Add 3x hidden 2D convolutions LSTM layers    \n",
    "# Begin with 2D convolutional LSTM layer\n",
    "seq.add(keras.layers.ConvLSTM2D(filters=1, kernel_size=(1, 1), # (1, 1)\n",
    "                                #input_shape=(None, 40, 40, 1),\n",
    "                                padding='same', return_sequences=True,  data_format='channels_last')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "seq.compile(loss='mean_squared_error', \n",
    "            optimizer=sgd)\n",
    "\n",
    "log_dir = \"/home/hanna/MS/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "#%tensorboard --logdir '/home/hanna/MS/logs/fit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 186 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "history = seq.fit(x=v, y=w, batch_size=32, epochs=10, verbose=1, callbacks=None, \n",
    "                  validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, \n",
    "                  sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, \n",
    "                  validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, val = True):\n",
    "    \"\"\" Plot history in \"\"\"\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    if val:\n",
    "        plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history, val = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import plot_model\n",
    "#plot_model(model, to_file=save_dir+'model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model1.compile(\n",
    "      loss = \"mse\",\n",
    "      metrics = [\"mae\"],\n",
    "      optimizer = \"adam\"\n",
    ")\n",
    "import datetime\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "history = model1.fit(x,\n",
    "                    y,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 4,\n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[tensorboard_callback]\n",
    "                    )\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7a043b5ce57d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvLSTM2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#from keras.layers.normalization import BatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from keras.layers import keras.layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from tensorkeras.models import Sequential\n",
    "from keras.layers import ConvLSTM2D\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "#from keras.layers import keras.layers.\n",
    "\n",
    "def build_model(filters = [256, 128, 64], kernal = [5, 5, 3], input_shape = (4, 4, 35, 60)):\n",
    "    model =  Sequential()\n",
    "\n",
    "    # Begin with 2D convolutional LSTM layer\n",
    "    for i, filter_ in enumerate( filters ):\n",
    "        k_s = kernal[i]\n",
    "        \n",
    "        if i == 0:\n",
    "            model.add(ConvLSTM2D(filters = filter_, kernel_size = (k_s, k_s),\n",
    "                                 input_shape = input_shape,\n",
    "                                 padding = 'same', return_sequences=True, data_format='channels_first')) \n",
    "        else:\n",
    "            model.add(ConvLSTM2D(filters = filter_, kernel_size =  (k_s, k_s),\n",
    "                                              #input_shape = (256, 35, 60, 1),\n",
    "                                              padding='same', return_sequences=True,  data_format='channels_first')) \n",
    "\n",
    "    # Add 3x hidden 2D convolutions LSTM layers    \n",
    "    # Begin with 2D convolutional LSTM layer\n",
    "    # Blir dette rett da..?\n",
    "    model.add(keras.layers.ConvLSTM2D(filters=1, kernel_size=(1, 1),\n",
    "                                      #input_shape=(None, 40, 40, 1),\n",
    "                                      padding='same', return_sequences=True,  data_format='channels_first')) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f8070d3fca51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional_recurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvLSTM2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from keras.layers import keras.layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model =  Sequential()\n",
    "\n",
    "# Begin with 2D convolutional LSTM layer\n",
    "model.add(ConvLSTM2D(filters = 256, kernel_size = (5, 5),\n",
    "                     input_shape = (4, 4, 35, 60),\n",
    "                     padding = 'same', return_sequences=True, data_format='channels_first')) \n",
    "\n",
    "model.add(keras.layers.ConvLSTM2D(filters = 128, kernel_size = (5, 5),\n",
    "                                  #input_shape = (256, 35, 60, 1),\n",
    "                                  padding='same', return_sequences=True,  data_format='channels_first')) \n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(keras.layers.ConvLSTM2D(filters=64, kernel_size=(3, 3),\n",
    "                                  #input_shape=(None, 35, 60, 1),\n",
    "                                  padding='same', return_sequences=True,  data_format='channels_first')) \n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add 3x hidden 2D convolutions LSTM layers    \n",
    "# Begin with 2D convolutional LSTM layer\n",
    "model.add(keras.layers.ConvLSTM2D(filters=1, kernel_size=(3, 3),\n",
    "                                  #input_shape=(None, 40, 40, 1),\n",
    "                                  padding='same', return_sequences=True,  data_format='channels_first')) \n",
    "\n",
    "# Prepare model for training\n",
    "model.compile(\n",
    "      loss = \"mse\",\n",
    "      metrics = [\"mae\"],\n",
    "      optimizer = \"adam\"\n",
    ")\n",
    "\n",
    "history2 = model.fit(x,\n",
    "                    y,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 40,\n",
    "                    validation_split = 0.2, \n",
    "                    callbacks=[tensorboard_callback]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "def save_json(model, m = \"model.json\"):    \n",
    "    \"\"\" Storing a config / architecture of the model.\"\"\"\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(m, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    return\n",
    "\n",
    "def load_model(m = 'model.json'):\n",
    "    \"\"\" Loading a stored config/ architecture  to a model. \"\"\"\n",
    "    # load json and create model\n",
    "    json_file = open(m, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_storage = '/home/hanna/MS/stored_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json( model, model_storage + \"example_keras.json\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(  model_storage + \"example_keras.json\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_storage + \"example_keras.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_storage + \"example_keras.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your own/ Custumize metrics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
