{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import timeit\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hvilke filer er ferdig downsampled med ikke kjørt regresjon på "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5823"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_files = glob.glob('/home/hanna/lagrings/ar_data/*.nc')\n",
    "len(lh_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_name(model_type, bias, transform, sigmoid, order):\n",
    "    name = model_type\n",
    "    if bias:\n",
    "        name += '-B'\n",
    "    if transform:\n",
    "        name += '-T'\n",
    "    if sigmoid:\n",
    "        name += '-S'\n",
    "    name += '-{}'.format(order)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = generate_model_name(model_type='AR', bias=True, transform=False, sigmoid=False, order=5)\n",
    "m1 = generate_model_name(model_type='*', bias=False, transform=False, sigmoid=False, order=5)\n",
    "m2 = generate_model_name(model_type='*', bias=True, transform=False, sigmoid=True, order=5)\n",
    "m3 = generate_model_name(model_type='*', bias=False, transform=True, sigmoid=True, order=5)\n",
    "m4 = generate_model_name(model_type='*', bias=False, transform=True, sigmoid=False, order=5)\n",
    "m5 = generate_model_name(model_type='*', bias=False, transform=False, sigmoid=True, order=5)\n",
    "\n",
    "model_names = [m0, m1, m2, m3, m4, m5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AR-B-5'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1564"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finished_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/hanna/lagrings/ar_data/all_vars_lat_lon_30.0_-15.0.nc',\n",
       " '/home/hanna/lagrings/ar_data/all_vars_lat_lon_30.0_-14.75.nc',\n",
       " '/home/hanna/lagrings/ar_data/all_vars_lat_lon_30.0_-14.5.nc']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/hanna/lagrings/results/ar/performance_AR-B-5-o2_-15.0_30.0.nc',\n",
       " '/home/hanna/lagrings/results/ar/performance_AR-B-5-o2_-14.75_30.0.nc',\n",
       " '/home/hanna/lagrings/results/ar/performance_AR-B-5-o2_-14.5_30.0.nc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished_files[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# if not in finished but in lh add file to list we will regridd in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_files = glob.glob(f'/home/hanna/lagrings/results/ar/*performance*{m0}*o{2}*')\n",
    "\n",
    "crop_files_finished = []\n",
    "for fil in finished_files:\n",
    "    splits = fil.split('_')\n",
    "    lat = splits[-2]\n",
    "    lon = splits[-1][:-3]\n",
    "    crop_files_finished.append(f'{lon}_{lat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5823"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lh_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4259"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_length = len(lh_files) - len(crop_files_finished)\n",
    "exp_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_regridd = []\n",
    "#crop_files = []\n",
    "for fil in lh_files:\n",
    "    splits = fil.split('_')\n",
    "    lat = splits[-2]\n",
    "    lon = splits[-1][:-3]\n",
    "    search_key = f'{lat}_{lon}'\n",
    "\n",
    "    if not search_key in crop_files_finished:\n",
    "        list_to_regridd.append(fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4259"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_to_regridd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil = lh_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = fil.split('_')\n",
    "lat = splits[-2]\n",
    "lon = splits[-1][:-3]\n",
    "search_key = f'{lat}_{lon}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('30.0', '-15.0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fil in list_to_regridd:\n",
    "    \n",
    "    splits = fil.split('_')\n",
    "    latitude = splits[-2]\n",
    "    longitude = splits[-1][:-3]\n",
    "\n",
    "    data = xr.open_dataset(fil)\n",
    "    \n",
    "    explain = explaination.copy()\n",
    "    tr_explain = tr_e.copy()\n",
    "\n",
    "    for o in range(0, order +1):\n",
    "        name    = full_name+'-L{}'.format(o)\n",
    "        tr_name = full_name_tr+'-L{}'.format(o)\n",
    "\n",
    "\n",
    "        if o > 0:\n",
    "            explain.append('O{}'.format(o))\n",
    "            tr_explain.append('O{}'.format(o))\n",
    "        start_time = timeit()\n",
    "\n",
    "        X_train, y_train = dataset_to_numpy_order(dataset = data.sel(time = slice('2004', '2013')),\n",
    "                        order = order,  bias = bias)\n",
    "        #print(X_train[0, :])\n",
    "        X_test, y_test   = dataset_to_numpy_order(dataset = data.sel(time = slice('2014', '2018')),\n",
    "                        order = order,  bias = bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_files_to_regridd(model_name, path):\n",
    "    finished_files = glob.glob(path + f'results/ar/*performance*{model_name}*o{2}*')\n",
    "    lh_files = glob.glob(path+'ar_data/*.nc')\n",
    "    # Get search string from trained models. \n",
    "    crop_files_finished = []\n",
    "    for fil in finished_files:\n",
    "        splits = fil.split('_')\n",
    "        lat = splits[-2]\n",
    "        lon = splits[-1][:-3]\n",
    "        crop_files_finished.append(f'{lon}_{lat}')\n",
    "        \n",
    "    list_to_regridd = []\n",
    "    # Crop_files = []\n",
    "    for fil in lh_files:\n",
    "        splits = fil.split('_')\n",
    "        lat = splits[-2]\n",
    "        lon = splits[-1][:-3]\n",
    "        search_key = f'{lat}_{lon}'\n",
    "\n",
    "        if not search_key in crop_files_finished:\n",
    "            list_to_regridd.append(fil)\n",
    "    return list_to_regridd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_regridd = get_list_of_files_to_regridd(model_name = m0, path = '/home/hanna/lagrings/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4263"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_to_regridd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from timeit import timeit\n",
    "\n",
    "from utils import (dataset_to_numpy, dataset_to_numpy_order,\n",
    "                                         dataset_to_numpy_order_traditional_ar)\n",
    "\n",
    "from utils import (mean_squared_error, mean_absolute_error,\n",
    "                                         r2_score, fit_pixel,\n",
    "                                         predict_pixel,\n",
    "                                         accumulated_squared_error,\n",
    "                                         sigmoid, inverse_sigmoid)\n",
    "\n",
    "def drop_nans(X, y):\n",
    "    a = np.concatenate([X, y], axis = 1)\n",
    "    B = a[~np.isnan(a).any(axis = 1)]\n",
    "    X = B[:, :-1]\n",
    "    y = B[:, -1, np.newaxis] # not tested\n",
    "    return X, y\n",
    "\n",
    "def generate_model_name(model_type, bias, transform, sigmoid, order):\n",
    "    name = model_type\n",
    "    if bias:\n",
    "        name += '-B'\n",
    "    if transform:\n",
    "        name += '-T'\n",
    "    if sigmoid:\n",
    "        name += '-S'\n",
    "    name += '-{}'.format(order)\n",
    "    return name\n",
    "\n",
    "def get_config_from_model_name(name):\n",
    "    \"\"\"returns dict with config\"\"\"\n",
    "    splits = name.split('-')\n",
    "\n",
    "    bias = False\n",
    "    transform = False\n",
    "    sigmoid = False\n",
    "\n",
    "    if 'B' in splits:\n",
    "        bias = True\n",
    "    if 'T' in splits:\n",
    "        transform = True\n",
    "    if 'S' in splits:\n",
    "        sigmoid = True\n",
    "\n",
    "    order      = int(splits[-1])\n",
    "    model_type = splits[0]\n",
    "    temp_dict  = {'transform'  : transform,\n",
    "                'sigmoid'    : sigmoid,\n",
    "                'order'      : order,\n",
    "                'start'      : '2004',\n",
    "                'stop'       : '2013',\n",
    "                'test_start' : '2014',\n",
    "                'test_stop'  : '2018',\n",
    "                'bias'       :  bias,\n",
    "                'type'       :  model_type}\n",
    "\n",
    "    return temp_dict\n",
    "\n",
    "def get_list_of_files_to_regridd(model_name, path):\n",
    "    finished_files = glob.glob(path + f'results/ar/*performance*{model_name}*o{2}*')\n",
    "    lh_files = glob.glob(path+'ar_data/*.nc')\n",
    "    # Get search string from trained models. \n",
    "    crop_files_finished = []\n",
    "    for fil in finished_files:\n",
    "        splits = fil.split('_')\n",
    "        lat = splits[-2]\n",
    "        lon = splits[-1][:-3]\n",
    "        crop_files_finished.append(f'{lon}_{lat}')\n",
    "        \n",
    "    list_to_regridd = []\n",
    "    # Crop_files = []\n",
    "    for fil in lh_files:\n",
    "        splits = fil.split('_')\n",
    "        lat = splits[-2]\n",
    "        lon = splits[-1][:-3]\n",
    "        search_key = f'{lat}_{lon}'\n",
    "\n",
    "        if not search_key in crop_files_finished:\n",
    "            list_to_regridd.append(fil)\n",
    "    return list_to_regridd\n",
    "\n",
    "def transform_X(X, lat, lon, data, order):\n",
    "    X_train = np.zeros(X.shape)\n",
    "    variables = ['q', 't2m', 'r', 'sp']\n",
    "\n",
    "    if len(variables) > 0:\n",
    "        for j, var in enumerate(variables):\n",
    "            m = data[var].sel(latitude = lat, longitude = lon)['mean'].values\n",
    "            s = data[var].sel(latitude = lat, longitude = lon)['std'].values\n",
    "            X_train[:, j] = (X[:, j]- m)/s\n",
    "    else:\n",
    "        j = 0\n",
    "    if order > 0:\n",
    "        var = 'tcc'\n",
    "        for k in range(order):\n",
    "            m = data[var].sel(latitude = lat, longitude = lon)['mean'].values\n",
    "            s = data[var].sel(latitude = lat, longitude = lon)['std'].values\n",
    "            # Something wierd with the rotation of cloud cover values\n",
    "            X_train[:, k+j+1] = (X[:, j+k+1]- m)/s\n",
    "    return X_train\n",
    "\n",
    "\n",
    "def inverse_sigmoid(x):\n",
    "    \"\"\"Also known as the logit function. Expression np.log(x/(1-x).\n",
    "    Use to transform the response to be in the range (-inf, +inf).\n",
    "\n",
    "    Parameteres\n",
    "    -------------------\n",
    "    x : array-like\n",
    "      Vector containing the values.\n",
    "\n",
    "    Returnes\n",
    "    --------------------\n",
    "    _ : array-like\n",
    "      The inverse sigmoid transform of x\n",
    "    \"\"\"\n",
    "    return np.log(x/(1 - x + 0.1))\n",
    "\n",
    "\n",
    "def train_ar_model(transform=False, bias=False, sig=False, order=0, overwrite_results = True):\n",
    "    path_transform = '/home/hanna/lagrings/results/stats/2014-01-01_2018-12-31/'\n",
    "    path = '/home/hanna/lagrings/ar_data/'\n",
    "\n",
    "    if transform and bias:\n",
    "        print('Not valid model....')\n",
    "        raise OSError('Not valid model config')\n",
    "\n",
    "    # path_transform = '/home/hanna/lagrings/results/stats/2014-01-01_2018-12-31/'\n",
    "    # path = '/home/hanna/lagrings/ar_data/'\n",
    "\n",
    "    lagr_path = '/uio/lagringshotell/geofag/students/metos/hannasv/'\n",
    "    path_transform = '{}results/stats/2014-01-01_2018-12-31'.format(lagr_path)\n",
    "\n",
    "    path = '{}ar_data/'.format(lagr_path)\n",
    "    path_ar_results = '{}/results/ar/'.format(lagr_path)\n",
    "\n",
    "    latitude  = 30.0\n",
    "    longitude = 5.25\n",
    "    SPATIAL_RESOLUTION = 0.25\n",
    "\n",
    "    latitudes =  np.arange(30.0, 50.0 +SPATIAL_RESOLUTION, step = SPATIAL_RESOLUTION)\n",
    "    longitudes = np.arange(-15, 25+SPATIAL_RESOLUTION, step = SPATIAL_RESOLUTION)\n",
    "    base = '{}/results/stats/2014-01-01_2018-12-31/'.format(lagr_path)\n",
    "\n",
    "    if transform:\n",
    "        ds_tcc = xr.open_dataset(base + 'stats_pixel_tcc_all.nc')\n",
    "        ds_r = xr.open_dataset(base +'stats_pixel_r_all.nc')\n",
    "        ds_q = xr.open_dataset(base+'stats_pixel_q_all.nc')\n",
    "        ds_t2m = xr.open_dataset(base+'stats_pixel_t2m_all.nc')\n",
    "        ds_sp = xr.open_dataset(base+'stats_pixel_sp_all.nc')\n",
    "\n",
    "        stats_data = {'q':ds_q, 't2m': ds_t2m, 'r': ds_r, 'sp': ds_sp, 'tcc': ds_tcc}\n",
    "\n",
    "    explaination = ['q', 't2m', 'r', 'sp']\n",
    "    tr_e = []\n",
    "    tr_index = 4\n",
    "\n",
    "    if bias:\n",
    "        explaination.append('bias')\n",
    "        tr_e.append('bias')\n",
    "        #tr_index +=1\n",
    "\n",
    "    full_name = generate_model_name('AR', bias, transform, sig, order)\n",
    "    config = get_config_from_model_name(full_name)\n",
    "\n",
    "    full_name_tr = generate_model_name('TR', bias, transform, sig, order)\n",
    "    tr_config = get_config_from_model_name(full_name_tr)\n",
    "    \n",
    "    list_to_regridd = get_list_of_files_to_regridd(model_name, path)\n",
    "    print('Detected {} num files to regridd.'.format(len( list_to_regridd ))) \n",
    "    for fil in list_to_regridd:\n",
    "\n",
    "        splits = fil.split('_')\n",
    "        latitude = splits[-2]\n",
    "        longitude = splits[-1][:-3]\n",
    "\n",
    "        data = xr.open_dataset(fil)\n",
    "\n",
    "        explain = explaination.copy()\n",
    "        tr_explain = tr_e.copy()\n",
    "\n",
    "        for o in range(0, order +1):\n",
    "            name    = full_name+'-L{}'.format(o)\n",
    "            tr_name = full_name_tr+'-L{}'.format(o)\n",
    "\n",
    "            w_filename        = '{}weights_{}_{}_{}.nc'.format(path_ar_results, name, longitude, latitude)\n",
    "            p_filename        = '{}performance_{}_{}_{}.nc'.format(path_ar_results, name, longitude, latitude)\n",
    "            data = xr.open_dataset(path+fil)\n",
    "            if o > 0:\n",
    "                explain.append('O{}'.format(o))\n",
    "                tr_explain.append('O{}'.format(o))\n",
    "            start_time = timeit()\n",
    "\n",
    "            X_train, y_train = dataset_to_numpy_order(dataset = data.sel(time = slice('2004', '2013')),\n",
    "                            order = order,  bias = bias)\n",
    "            #print(X_train[0, :])\n",
    "            X_test, y_test   = dataset_to_numpy_order(dataset = data.sel(time = slice('2014', '2018')),\n",
    "                            order = order,  bias = bias)\n",
    "            #print('transform {}'.format(transform))\n",
    "            #print(bias)\n",
    "            if transform: # and not bias):# or (not transform and bias):\n",
    "                X_train = transform_X(X_train, lat = latitude, lon=longitude, data=stats_data, order=o)\n",
    "                X_test = transform_X(X_test, lat = latitude, lon=longitude, data=stats_data, order=o)\n",
    "            #else:\n",
    "            #    print('Not valid model....')\n",
    "            #    raise OSError('Not valid model config')\n",
    "\n",
    "            if sig:\n",
    "                y_train = inverse_sigmoid(y_train)\n",
    "                y_test  = inverse_sigmoid(y_test)\n",
    "\n",
    "            name    = full_name+'-o{}'.format(o)\n",
    "            tr_name = full_name_tr+'-o{}'.format(o)\n",
    "\n",
    "            eval_dict    = {}\n",
    "            eval_tr_dict = {}\n",
    "            weights_dict = {}\n",
    "            weights_tr_dict = {}\n",
    "\n",
    "            Xtr, ytr =  drop_nans(X_train[:, :int(tr_index+o)], y_train)\n",
    "            Xte, yte =  drop_nans(X_test[:, :int(tr_index+o)], y_test)\n",
    "\n",
    "            if sig:\n",
    "                yte = sigmoid(yte)\n",
    "                ytr = sigmoid(ytr)\n",
    "\n",
    "            if np.isnan(yte).any():\n",
    "                print('Warning nans detected in training data')\n",
    "\n",
    "            if np.isnan(ytr).any():\n",
    "                print('Warning nans detected in test data')\n",
    "\n",
    "            if o > 0:\n",
    "                # updatig predictors\n",
    "                Tr_Xtr =  Xtr[:, tr_index:]\n",
    "                Tr_Xte =  Xte[:, tr_index:]\n",
    "                print(Tr_Xtr.shape)\n",
    "                coeffs_tr   = fit_pixel(Tr_Xtr, ytr)\n",
    "\n",
    "                y_test_pred_tr  = predict_pixel(Tr_Xte, coeffs_tr)\n",
    "                y_train_pred_tr = predict_pixel(Tr_Xtr, coeffs_tr)\n",
    "\n",
    "                mse_test_tr  = mean_squared_error(y_test_pred_tr, yte)\n",
    "                mse_train_tr = mean_squared_error(y_train_pred_tr, ytr)\n",
    "\n",
    "                mae_test_tr  = mean_absolute_error(y_test_pred_tr, yte)\n",
    "                mae_train_tr = mean_absolute_error(y_train_pred_tr, ytr)\n",
    "\n",
    "            ############# Fitting\n",
    "            coeffs      = fit_pixel(Xtr, ytr)\n",
    "\n",
    "            y_test_pred  = predict_pixel(Xte, coeffs)\n",
    "            y_train_pred = predict_pixel(Xtr, coeffs)\n",
    "\n",
    "            ################ Evaluation\n",
    "            mse_test  = mean_squared_error(y_test_pred, yte)\n",
    "            mse_train = mean_squared_error(y_train_pred, ytr)\n",
    "\n",
    "            mae_test  = mean_squared_error(y_test_pred, yte)\n",
    "            mae_train = mean_squared_error(y_train_pred, ytr)\n",
    "\n",
    "            ##################### Adding the autoregressive model\n",
    "            #print(coeffs)\n",
    "            #print(explaination)\n",
    "            weights_dict['coeffs'] = (['weights'], coeffs.flatten())  # 'latitude', 'longitude',\n",
    "\n",
    "            eval_dict['mse_test']  = mse_test[0]   #(['latitude', 'longitude'],)\n",
    "            eval_dict['mse_train'] = mse_train[0]\n",
    "\n",
    "            eval_dict['mae_test']  = mae_test[0]   #(['latitude', 'longitude'], )\n",
    "            eval_dict['mae_train'] = mae_train[0]  #(['latitude', 'longitude'], )\n",
    "\n",
    "            num_test_samples  = len(yte)\n",
    "            num_train_samples = len(ytr)\n",
    "\n",
    "            eval_dict['num_test_samples']  = num_test_samples  # (['latitude', 'longitude'], )\n",
    "            eval_dict['num_train_samples'] = num_train_samples # (['latitude', 'longitude'], )\n",
    "\n",
    "            eval_dict.update(config)\n",
    "            weights_dict.update(config)\n",
    "\n",
    "            ###################### Adding traditional model\n",
    "            if o > 0:\n",
    "                weights_tr_dict['coeffs'] = (['weights'], coeffs_tr.flatten())  # 'latitude', 'longitude',\n",
    "                print(weights_tr_dict)\n",
    "                print(tr_explain)\n",
    "                eval_tr_dict['mse_test']  = mse_test_tr[0]   #(['latitude', 'longitude'],)\n",
    "                eval_tr_dict['mse_train'] = mse_train_tr[0]\n",
    "\n",
    "                eval_tr_dict['mae_test']  = mae_test_tr[0]   #(['latitude', 'longitude'], )\n",
    "                eval_tr_dict['mae_train'] = mae_train_tr[0]  #(['latitude', 'longitude'], )\n",
    "\n",
    "                num_test_samples  = len(yte)\n",
    "                num_train_samples = len(ytr)\n",
    "\n",
    "                eval_tr_dict['num_test_samples']  = num_test_samples  # (['latitude', 'longitude'], )\n",
    "                eval_tr_dict['num_train_samples'] = num_train_samples # (['latitude', 'longitude'], )\n",
    "\n",
    "                eval_tr_dict.update(tr_config)\n",
    "                weights_tr_dict.update(tr_config)\n",
    "\n",
    "                w_tr_filename        = '{}/weights_{}_{}_{}.nc'.format(path_ar_results, tr_name, longitude, latitude)\n",
    "                p_tr_filename        = '{}/performance_{}_{}_{}.nc'.format(path_ar_results, tr_name, longitude, latitude)\n",
    "\n",
    "                ds = xr.Dataset(weights_tr_dict, coords={'latitude':  (['latitude'],  [latitude]),\n",
    "                                            'longitude': (['longitude'], [longitude]),\n",
    "                                            'weights':   (['weights'],   tr_explain )\n",
    "                                            })\n",
    "                ds.to_netcdf(w_tr_filename)\n",
    "\n",
    "                ds = xr.Dataset(eval_tr_dict, coords={'latitude': (['latitude'],   [latitude]),\n",
    "                                         'longitude': (['longitude'], [longitude])\n",
    "                                         })\n",
    "                ds.to_netcdf(p_tr_filename)\n",
    "\n",
    "            stop_time = timeit()\n",
    "            #print(stop_time - start_time)\n",
    "            eval_dict['time_elapsed_seconds'] = (stop_time - start_time) #(['latitude', 'longitude'], )\n",
    "\n",
    "            w_filename        = '{}weights_{}_{}_{}.nc'.format(path_ar_results, name, longitude, latitude)\n",
    "            p_filename        = '{}performance_{}_{}_{}.nc'.format(path_ar_results, name, longitude, latitude)\n",
    "            ds = xr.Dataset(weights_dict, coords={'latitude':  (['latitude'],  [latitude]),\n",
    "                                    'longitude': (['longitude'], [longitude]),\n",
    "                                    'weights':   (['weights'],   explain )\n",
    "                                    })\n",
    "            ds.to_netcdf(w_filename)\n",
    "\n",
    "            ds = xr.Dataset(eval_dict, coords={'latitude': (['latitude'],   [latitude]),\n",
    "                                 'longitude': (['longitude'], [longitude])\n",
    "                                 })\n",
    "            ds.to_netcdf(p_filename)\n",
    "            print('finished calibrating bias {}, sigmoid {}, Transform {}, order/Lag {} - ({}, {})'.format(bias, sig, transform, o, longitude, latitude))\n",
    "        else:\n",
    "            print('Model config already calibrated bias {}, sigmoid {}, Transform {}, order/Lag {} - ({}, {})'.format(bias, sig, transform, o, longitude, latitude))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
