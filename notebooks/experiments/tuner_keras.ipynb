{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparam tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "\n",
    "from sclouds.helpers import get_lon_array, get_lat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-42a074ee9a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m lr = hp.Float(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmin_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LOG'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hp' is not defined"
     ]
    }
   ],
   "source": [
    "lr = hp.Float(\n",
    "    'learning_rate',\n",
    "    min_value=1e-5,\n",
    "    max_value=1e-2,\n",
    "    sampling='LOG',\n",
    "    default=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters=hp.Choice('num_filters',\n",
    "                values=[32, 64, 218],\n",
    "                default=64,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom R2-score metrics for keras backend\n",
    "def r2_keras(y_true, y_pred):\n",
    "    import keas.backend as kb\n",
    "    SS_res =  kb.sum(kb.square(y_true - y_pred)) \n",
    "    SS_tot = kb.sum(kb.square(y_true - kb.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + kb.epsilon()) )\n",
    "\n",
    "def keras_custom_loss_function(y_actual, y_predict):\n",
    "    \"\"\"Custum keras loss function, accumulated squared error. \"\"\"\n",
    "    import keas.backend as kb\n",
    "    return np.square(np.subtract(y_actual, y_predict)).sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO figure out how to add the custom loss function later\n",
    "\n",
    "losses = hp.Choice(\n",
    "            'loss',\n",
    "            values=['mean_squared_error', 'mean_absolute_error'],\n",
    "            default='mean_squared_error',\n",
    "        ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense(\n",
    "    units=hp.Int(\n",
    "        'units',\n",
    "        min_value=32,\n",
    "        max_value=512,\n",
    "        step=32,\n",
    "        default=128\n",
    "    ),\n",
    "    activation=hp.Choice(\n",
    "        'dense_activation',\n",
    "        values=['relu', 'tanh', 'sigmoid'],\n",
    "        default='relu'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.Int('units',\n",
    "        min_value=32,\n",
    "        max_value=512,\n",
    "        step=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import HyperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import HyperModel\n",
    "from sclouds.helpers import get_lon_array, get_lat_array\n",
    "\n",
    "n_lon = len(get_lon_array())\n",
    "n_lat = len(get_lat_array())\n",
    "\n",
    "class HyperConvLSTM(HyperModel):\n",
    "    \n",
    "    DATA_FORMAT        = 'channels_last'\n",
    "    PADDING            = 'same'\n",
    "    RETURN_SEQUENCE    = True\n",
    "    NUM_INPUT_VARS     = 4\n",
    "    OUTPUT_KERNEL_SIZE = 1\n",
    "    OUTPUT_FILTER      = 1\n",
    "\n",
    "    def __init__(self, num_hidden_layers, seq_length):\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def build(self, hp):\n",
    "        model =  Sequential()\n",
    "        # Adding the first layer\n",
    "\n",
    "        model.add(ConvLSTM2D(filters= hp.Int('filters',\n",
    "                                        min_value=32,\n",
    "                                        max_value=512,\n",
    "                                        step=32),                             \n",
    "                           kernel_size = (3, 3),\n",
    "                           input_shape = (self.seq_length, n_lat, n_lon, self.NUM_INPUT_VARS),\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           padding = self.PADDING,\n",
    "                           return_sequences=self.RETURN_SEQUENCE,\n",
    "                           data_format=self.DATA_FORMAT))\n",
    "\n",
    "        #prev_filter = filters[0]\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            # Begin with 3D convolutional LSTM layer\n",
    "            model.add(keras.layers.ConvLSTM2D(filters= hp.Int('filters',\n",
    "                                                        min_value=32,\n",
    "                                                        max_value=512,\n",
    "                                                        step=32),\n",
    "                                            kernel_size=(3, 3),\n",
    "                                            kernel_initializer='glorot_uniform',\n",
    "                                            padding = self.PADDING,\n",
    "                                            return_sequences = self.RETURN_SEQUENCE,\n",
    "                                            data_format = self.DATA_FORMAT))\n",
    "        # Adding the last layer\n",
    "        model.add(keras.layers.ConvLSTM2D(filters = self.OUTPUT_FILTER,\n",
    "                                          kernel_size = (1, 1),\n",
    "                                          kernel_initializer='glorot_uniform',\n",
    "                                          padding = self.PADDING,\n",
    "                                          return_sequences = self.RETURN_SEQUENCE,\n",
    "                                          data_format = self.DATA_FORMAT))\n",
    "\n",
    "        \n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Choice('learning_rate',\n",
    "                          values=[1e-2, 1e-3, 1e-4])),\n",
    "            loss=hp.Choice('loss',\n",
    "                            values=['mean_squared_error', 'mean_absolute_error'],\n",
    "                            default='mean_squared_error'),\n",
    "            metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*help(RandomSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from sclouds.helpers import path_convlstm_results\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization\n",
    "\n",
    "from sclouds.helpers import path_read_data\n",
    "from sclouds.io.utils import dataset_to_numpy, dataset_to_numpy_grid, dataset_to_numpy_order, get_xarray_dataset_for_period\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hanna/lagrings/results/convlstm/'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_convlstm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner import HyperParameters\n",
    "\n",
    "hp = HyperParameters()\n",
    "hypermodel = HyperConvLSTM(num_hidden_layers = 2, seq_length= 4)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "        hypermodel,\n",
    "        objective='mean_squared_error',\n",
    "        max_trials=10,\n",
    "        allow_new_entries = True, \n",
    "        directory=path_convlstm_results,\n",
    "        project_name='test_hyperparameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num files 5\n"
     ]
    }
   ],
   "source": [
    "from sclouds.io.utils import get_xarray_dataset_for_period, dataset_to_numpy_grid\n",
    "data = get_xarray_dataset_for_period(start = '2012-01-01', stop = '2012-01-31')\n",
    "X, y = dataset_to_numpy_grid(data, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, lat, lon, num_vars = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples/seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = X.reshape((int(num_samples/seq_length), seq_length, lat, lon, 4))\n",
    "y = y.reshape((int(num_samples/seq_length), seq_length, lat, lon, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4020079573af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mval_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlast_train_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mval_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "val_split = 0.2\n",
    "\n",
    "num_samples, seq_length, _, _, num_vars = X.shape\n",
    "last_train_idx = int((1-val_split)*num_samples)\n",
    "\n",
    "X_train = X[:last_train_idx, :, :, :]\n",
    "y_train = y[:last_train_idx, :,]\n",
    "\n",
    "X_test = X[last_train_idx:, :, :, :]\n",
    "y_test = y[last_train_idx:, :,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37 samples, validate on 149 samples\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train,\n",
    "             epochs=5,\n",
    "             validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.sequential.Sequential at 0x7fd3d41ab7b8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x7fd3d41ab438>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
