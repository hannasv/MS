{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-52cf4a8586db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxarray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "#import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\" Inspired by https://github.com/automan000/Convolution_LSTM_PyTorch. \"\"\"\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, GPU = False):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        assert hidden_channels % 2 == 0 # figure out what this does\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_features = 4\n",
    "\n",
    "        self.padding = int((kernel_size - 1) / 2)\n",
    "\n",
    "        \"\"\"\n",
    "        i - input gate\n",
    "        f - forget gate\n",
    "        c - (?) gate\n",
    "        o - output gate\n",
    "\n",
    "        les her https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # TWO D convolution since we have a grid.\n",
    "        # Why the bias in a certain way.\n",
    "        self.Wxi = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whi = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxf = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whf = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxc = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whc = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxo = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Who = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "\n",
    "        self.Wci = None\n",
    "        self.Wcf = None\n",
    "        self.Wco = None\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
    "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
    "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
    "        ch = co * torch.tanh(cc)\n",
    "        return ch, cc\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden, shape):\n",
    "        if self.Wci is None:\n",
    "            self.Wci = Variable(torch.zeros(1, hidden, shape[0], shape[1])).cpu() #cuda() here\n",
    "            self.Wcf = Variable(torch.zeros(1, hidden, shape[0], shape[1])).cpu()#.cuda()\n",
    "            self.Wco = Variable(torch.zeros(1, hidden, shape[0], shape[1])).cpu()#.cuda()\n",
    "        else:\n",
    "            assert shape[0] == self.Wci.size()[2], 'Input Height Mismatched!'\n",
    "            assert shape[1] == self.Wci.size()[3], 'Input Width Mismatched!'\n",
    "        return (Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).cpu(), #cuda(),\n",
    "                Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).cpu() )#cuda())\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    # input_channels corresponds to the first input feature map\n",
    "    # hidden state is a list of succeeding lstm layers.\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, step=1, effective_step=[1]):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.input_channels = [input_channels] + hidden_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = len(hidden_channels)\n",
    "        self.step = step\n",
    "        self.effective_step = effective_step\n",
    "        self._all_layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            name = 'cell{}'.format(i)\n",
    "            cell = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_size)\n",
    "            setattr(self, name, cell)\n",
    "            self._all_layers.append(cell)\n",
    "\n",
    "    def forward(self, input):\n",
    "        internal_state = []\n",
    "        outputs = []\n",
    "        for step in range(self.step):\n",
    "            x = input\n",
    "            for i in range(self.num_layers):\n",
    "                # all cells are initialized in the first step\n",
    "                name = 'cell{}'.format(i)\n",
    "                if step == 0:\n",
    "                    bsize, _, height, width = x.size()\n",
    "                    (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i],\n",
    "                                                             shape=(height, width))\n",
    "                    internal_state.append((h, c))\n",
    "\n",
    "                # do forward\n",
    "                (h, c) = internal_state[i]\n",
    "                x, new_c = getattr(self, name)(x, h, c)\n",
    "                internal_state[i] = (x, new_c)\n",
    "            # only record effective steps\n",
    "            if step in self.effective_step:\n",
    "                outputs.append(x)\n",
    "\n",
    "        return outputs, (x, new_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ML_repo(start = \"2012-01-01\", stop = \"2013-01-01\", season = \"SON\", \n",
    "                  era_path = '/uio/lagringshotell/geofag/students/metos/hannasv/era_interim_data/'):\n",
    "    \"\"\"\n",
    "    Want all year put season to \"\".\n",
    "    \"\"\"\n",
    "    q   = xr.open_dataset(glob.glob(era_path + \"*_q_*\" + season +\".nc\")[0]).q.values\n",
    "    r   = xr.open_dataset(glob.glob(era_path + \"*_r_*\"+ season +\".nc\")[0]).r.values\n",
    "    tcc = xr.open_dataset(glob.glob(era_path + \"*tcc*\"+ season +\".nc\")[0]).tcc.values\n",
    "    sp  = xr.open_dataset(glob.glob(era_path + \"*sp*\"+ season +\".nc\")[0]).sp.values\n",
    "    t2m = xr.open_dataset(glob.glob(era_path + \"*t2m*\"+ season +\".nc\")[0]).t2m.values\n",
    "    assert np.shape(q) == np.shape(r) == np.shape(tcc) == np.shape(sp) == np.shape(t2m)\n",
    "    \n",
    "    nbr_times, nbr_lats, nbr_lon = np.shape(q)\n",
    "    train = []\n",
    "    true  = tcc\n",
    "    for i in range(nbr_times):\n",
    "        one_timestep = np.array([q[i], r[i], tcc[i], t2m[i] ])\n",
    "        #print(one_timestep.shape)\n",
    "        train.append(one_timestep)\n",
    "    return np.array(train), true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_ML_repo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "convlstm = ConvLSTM(input_channels=512, hidden_channels=[128, 64, 64, 32, 32], kernel_size = 3, step=5,\n",
    "                    effective_step=[4]).cpu()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "input = Variable(torch.randn(1, 512, 64, 32)).cpu()\n",
    "target = Variable(torch.randn(1, 32, 64, 32)).double().cpu()\n",
    "\n",
    "output = convlstm(input)\n",
    "output = output[0][0].double()\n",
    "res = torch.autograd.gradcheck(loss_fn, (output, target), eps=1e-6, raise_exception=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
