{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3c9d1bcc7294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#import cartopy.crs as ccrs*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "#import cartopy.crs as ccrs*\n",
    "import numpy as np\n",
    "#from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "#from utils.saverRestorer import SaverRestorer. Hvordan fikser jeg dette Severin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters useful stuff\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "DATA_PATH = '/uio/lagringshotell/geofag/students/metos/hannasv/era_interim_data/'\n",
    "MODEL_STORE_PATH = '/uio/lagringshotell/geofag/students/metos/hannasv/era_interim_data/storedModel/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-08, 1.33352143e-07, 1.77827941e-06, 2.37137371e-05,\n",
       "       3.16227766e-04, 4.21696503e-03, 5.62341325e-02, 7.49894209e-01,\n",
       "       1.00000000e+01])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-8, 1, num = 9, endpoint = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ML_repo(start = \"2012-01-01\", stop = \"2013-01-01\", season = \"SON\", \n",
    "                   era_path = '/uio/lagringshotell/geofag/students/metos/hannasv/era_interim_data/'):\n",
    "    \"\"\" Want all year put season to. \"\"\"\n",
    "    q   = xr.open_dataset(glob.glob(era_path + \"*_q_*\" + season +\".nc\")[0]).q.values\n",
    "    r   = xr.open_dataset(glob.glob(era_path + \"*_r_*\"+ season +\".nc\")[0]).r.values\n",
    "    tcc = xr.open_dataset(glob.glob(era_path + \"*tcc*\"+ season +\".nc\")[0]).tcc.values\n",
    "    sp  = xr.open_dataset(glob.glob(era_path + \"*sp*\"+ season +\".nc\")[0]).sp.values\n",
    "    t2m = xr.open_dataset(glob.glob(era_path + \"*t2m*\"+ season +\".nc\")[0]).t2m.values\n",
    "    assert np.shape(q) == np.shape(r) == np.shape(tcc) == np.shape(sp) == np.shape(t2m)\n",
    "    \n",
    "    nbr_times, nbr_lats, nbr_lon = np.shape(q)\n",
    "    train = []\n",
    "    true  = tcc # is wrong now\n",
    "    a, b, c = np.shape(true)\n",
    "    true = true.reshape(a, 1, b, c)\n",
    "    \n",
    "    for i in range(nbr_times):\n",
    "        one_timestep = np.array([q[i], r[i], sp[i], t2m[i] ])\n",
    "        train.append(one_timestep)\n",
    "\n",
    "    return np.array(train), true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, test_split = 0.2):\n",
    "    assert test_split < 1, 'test split is given as a decimal number, choose a number between 0, 1'\n",
    "    indx = int(len(data)*(1-test_split))\n",
    "    # returns train test\n",
    "    return data[:indx], data[indx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test = train_test_split(np.concatenate((train, test), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.ConvLSTM2DCell(filters, kernel_size, strides=(1, 1), padding='valid', \n",
    "                            data_format=None, dilation_rate=(1, 1), activation='tanh', \n",
    "                            recurrent_activation='hard_sigmoid', use_bias=True, \n",
    "                            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "                            bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "                            recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, \n",
    "                            recurrent_constraint=None, \n",
    "                            bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.reshape(546, 4, 4, 35, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(iter(train_loader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgs, steering_angle = next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "convlstm = ConvLSTM(input_channels = 4, hidden_channels = [256, 128, 64, 1], kernel_size = 5).cpu()\n",
    "optimizer = optim.RMSprop(convlstm.parameters(), lr=0.001, momentum=0.9)\n",
    "train, test = create_ML_repo()\n",
    "train_loader = DataLoader(dataset=np.concatenate((train, test), axis = 1), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLSTM(\n",
       "  (cell0): ConvLSTMCell(\n",
       "    (Wxi): Conv2d(4, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whi): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxf): Conv2d(4, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whf): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxc): Conv2d(4, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whc): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxo): Conv2d(4, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Who): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "  )\n",
       "  (cell1): ConvLSTMCell(\n",
       "    (Wxi): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whi): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxf): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whf): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxc): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whc): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxo): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Who): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "  )\n",
       "  (cell2): ConvLSTMCell(\n",
       "    (Wxi): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whi): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxf): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whf): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxc): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whc): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxo): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Who): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "  )\n",
       "  (cell3): ConvLSTMCell(\n",
       "    (Wxi): Conv2d(64, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whi): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxf): Conv2d(64, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whf): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxc): Conv2d(64, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Whc): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "    (Wxo): Conv2d(64, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (Who): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convlstm.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score = []\n",
    "nbr_epochs = 10\n",
    "\n",
    "for epoch in range(nbr_epochs):  \n",
    "    # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(iter(train_loader)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        \n",
    "        input = data[:, :-1, :, :]\n",
    "        target = data[:, -1, :, :]\n",
    "        a,b,c = target.shape\n",
    "        target = target.reshape(a, 1, b, c)\n",
    "        \n",
    "        inputs = torch.tensor(input)\n",
    "        clouds = torch.tensor(target)\n",
    "        \n",
    "        # Shape of input [samples (nbr days), nr hours, lat, lon, metvars]\n",
    "        input_  = Variable(inputs, requires_grad=True).cpu()\n",
    "        target_ = Variable(clouds, requires_grad=True).double().cpu()\n",
    "        \n",
    "        #output = output[0][0].double()\n",
    "        #res = torch.autograd.gradcheck(loss_fn, (output, target_), eps=1e-6, raise_exception=True)\n",
    "        #print(res)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, (x, new_c) = convlstm(inputs) # forward pass\n",
    "        \n",
    "        #print(output.shape)\n",
    "        print('x.shape {}'.format(x.shape))\n",
    "        print('clouds.shape {}'.format(clouds.shape))\n",
    "        loss = loss_fn(x, clouds)\n",
    "        loss.backward() # bakward step \n",
    "        optimizer.step() # optimize step\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss))\n",
    "        #running_loss = 0.0\n",
    "        score.append(running_loss)\n",
    "    \n",
    "    # Understanding plotting. \n",
    "    #plt.scatter(epoch, running_loss)\n",
    "    #plt.show()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(nbr_epochs), score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xavier_init_weights(m):\n",
    "    \"\"\" glorot initialization \"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "def LeCunn_init_weight(m):\n",
    "    stdv = 1. / math.sqrt(m.weight.size(1))\n",
    "    m.weight.data.uniform_(-stdv, stdv)\n",
    "    if m.bias is not None:\n",
    "        m.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\" Inspired by https://github.com/automan000/Convolution_LSTM_PyTorch. \"\"\"\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, GPU = False):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        #assert hidden_channels % 2 == 0 # figure out what this does\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_features = 4\n",
    "\n",
    "        self.padding = int((kernel_size - 1) / 2)\n",
    "\n",
    "        self.Wxi = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whi = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxf = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whf = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxc = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Whc = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        self.Wxo = nn.Conv2d(self.input_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=True)\n",
    "        self.Who = nn.Conv2d(self.hidden_channels, self.hidden_channels, self.kernel_size, 1, self.padding, bias=False)\n",
    "        # cell \n",
    "        self.Wci = None # input to sigmoid\n",
    "        self.Wcf = None # \n",
    "        self.Wco = None\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
    "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
    "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
    "        ch = co * torch.tanh(cc)\n",
    "        return ch, cc\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden, shape):\n",
    "        if self.Wci is None:\n",
    "            self.Wci = Variable(torch.zeros(1, hidden, shape[0], shape[1])).cpu() #cuda() here\n",
    "            self.Wcf = Variable(torch.zeros(1, hidden, shape[0], shape[1])).cpu()#.cuda()\n",
    "            self.Wco = Variable(torch.zeros(1, hidden, shape[0], shape[1])).cpu()#.cuda()\n",
    "        else:\n",
    "            assert shape[0] == self.Wci.size()[2], 'Input Height Mismatched!'\n",
    "            assert shape[1] == self.Wci.size()[3], 'Input Width Mismatched!'\n",
    "        return (Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).cpu(), #cuda(),\n",
    "                Variable(torch.zeros(batch_size, hidden, shape[0], shape[1])).cpu() )#cuda())\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    # input_channels corresponds to the first input feature map\n",
    "    # hidden state is a list of succeeding lstm layers.\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, step=1, effective_step=[1]):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.input_channels = [input_channels] + hidden_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = len(hidden_channels)\n",
    "        self.step = step\n",
    "        self.effective_step = effective_step\n",
    "        self._all_layers = []\n",
    "        for i in range(self.num_layers):\n",
    "            name = 'cell{}'.format(i)\n",
    "            cell = ConvLSTMCell(self.input_channels[i], self.hidden_channels[i], self.kernel_size)\n",
    "            setattr(self, name, cell)\n",
    "            self._all_layers.append(cell)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        internal_state = []\n",
    "        outputs = []\n",
    "        for step in range(self.step):\n",
    "            x = inputs\n",
    "            for i in range(self.num_layers):\n",
    "                # all cells are initialized in the first step\n",
    "                name = 'cell{}'.format(i)\n",
    "                \n",
    "                if step == 0:\n",
    "                    bsize, _, height, width = x.size()\n",
    "                    (h, c) = getattr(self, name).init_hidden(batch_size=bsize, hidden=self.hidden_channels[i],\n",
    "                                                             shape=(height, width))\n",
    "                    internal_state.append((h, c))\n",
    "\n",
    "                # do forward\n",
    "                (h, c) = internal_state[i]\n",
    "                x, new_c = getattr(self, name)(x, h, c)\n",
    "                internal_state[i] = (x, new_c)\n",
    "            # only record effective steps\n",
    "            if step in self.effective_step:\n",
    "                outputs.append(x)\n",
    "\n",
    "        return outputs, (x, new_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_ML_repo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(diff[0][0][:][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
