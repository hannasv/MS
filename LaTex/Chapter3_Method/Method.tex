\setcounter{chapter}{2}
\chapter{Methods}
Presenter innholder i kapitellet.

Tar det enkleste først også følger mer og mer detaljer..

\textbf{Formalism/methods: Discussion of the methods used and their basis/suitability.}\\ 

\section{Implementations, benchmarks and testing}

\section{Training and Testing split}
How is the split?
There is one obvious downside to using a separate training and test set.

\section{Statistical models}
\subsection{Linear model - AR model ts = 0} 
\subsection{AR model ts > 0}
\subsection{Confidence intervals on beta coefficients. }


\subsection{Why not shriking-metods? L1 and L2 penalties}
They only reduce noise, so if there is no noise present in theory their performance should not increase. Extra hyper parameter to introduce.


\section{Machine Learning models}
General what can machine learning models do.

\subsection{Do I need to describe the traditional dense neural net? If only to say that it doesn't work well for this purpose.}

Google elements of 

\subsection{Convolutional LSTM model}

\subsubsection{Convolutional operation}
\subsubsection{Reccurent neural nets }
First to loop information back. READ PHD from bergen to describe this one. 
\subsubsection{LSTM unit}

\subsection{TOwer Architecture}
\subsection{Architecture from presipitation nowcating.}


\section{Hyperparametrs}
\subsection{Normalization/Standardization}
If the data are not standardized, and the predictors are of vastly different scales, then this can lead to some very large β parameters which would be unfairly penalized when shrinkage is applied.

\section{Model evaluation - metrics}
\subsection{Accumulated MSE}


